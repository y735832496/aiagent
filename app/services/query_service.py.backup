from app.services.memory_context import MemoryContext
memory_context = MemoryContext()
import time
from typing import List, Dict, Any
from app.models.document import QueryRequest, QueryResponse, DocumentChunk
from app.utils.embedding_service import EmbeddingService
from app.utils.llm_service import LLMService
from app.services.storage_factory import StorageFactory
from app.services.langchain_service import LangChainRAGService
from app.config import settings

class QueryService:
    """æŸ¥è¯¢æœåŠ¡ - é›†æˆLangChain RAG"""
    
    def __init__(self):
        # ä¿ç•™åŸæœ‰æœåŠ¡ä½œä¸ºå¤‡ç”¨
        self.storage = StorageFactory.create_storage()
        self.embedding_service = EmbeddingService()
        self.llm_service = LLMService()
        
        # æ–°å¢LangChain RAGæœåŠ¡
        self.langchain_service = LangChainRAGService()
    
    async def query(self, request: QueryRequest) -> QueryResponse:
        """å¤„ç†æŸ¥è¯¢è¯·æ±‚ - ä½¿ç”¨LangChain RAG"""
        start_time = time.time()
        
        try:
            print(f"ğŸ” å¼€å§‹å¤„ç†æŸ¥è¯¢: '{request.query}'")
            
            # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
            top_k = request.top_k if request.top_k is not None else settings.top_k
            threshold = request.threshold if request.threshold is not None else settings.similarity_threshold
            
            print(f"ğŸ“Š å‚æ•°: top_k={top_k}, threshold={threshold}")
            
            # ä½¿ç”¨LangChain RAGæœåŠ¡
            print("ğŸ¤– ä½¿ç”¨LangChain RAGæœåŠ¡...")
            result = await self.langchain_service.query(
                query=request.query,
                top_k=top_k,
                threshold=threshold
            )
            print(f"result is :{result}")
            # å¤„ç†ç»“æœ
            answer = result.get("answer", "")
            print(f"answer is :{answer}")
            sources = result.get("sources", [])
            print(f"sources is :{sources}")
            confidence = result.get("confidence", 0.0)
            print(f"confidence is :{confidence}")
            # è½¬æ¢æ¥æºæ ¼å¼ä»¥ä¿æŒAPIå…¼å®¹æ€§
            formatted_sources = []
            for source in sources:
                formatted_source = {
                    "content_preview": source.get("content_preview", ""),
                    "similarity": 0.8,  # LangChainä¸ç›´æ¥æä¾›ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    "metadata": source.get("metadata", {})
                }
                formatted_sources.append(formatted_source)
            
            processing_time = time.time() - start_time
            print(f"âœ… LangChain RAGå¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.4f}ç§’")
            
            return QueryResponse(
                query=request.query,
                answer=answer,
                sources=formatted_sources if request.include_metadata else [],
                confidence=confidence,
                processing_time=processing_time,
                total_chunks_retrieved=len(sources)
            )
            
        except Exception as e:
            print(f"âŒ LangChain RAGæŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰æœåŠ¡: {e}")
            return await self._fallback_query(request, start_time)
    
    async def _fallback_query(self, request: QueryRequest, start_time: float) -> QueryResponse:
        """å›é€€åˆ°åŸæœ‰çš„æŸ¥è¯¢æœåŠ¡"""
        try:
            print("ğŸ”„ ä½¿ç”¨åŸæœ‰æŸ¥è¯¢æœåŠ¡...")
            
            # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
            top_k = request.top_k if request.top_k is not None else settings.top_k
            threshold = request.threshold if request.threshold is not None else settings.similarity_threshold
            
            # 1. å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
            print("ğŸ”„ æ­£åœ¨å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡...")
            query_vector = self.embedding_service.encode_single_text(request.query)
            print(f"âœ… æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {len(query_vector)}")
            
            # 2. å‘é‡æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£å—
            print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£å—...")
            similar_chunks = await self.storage.search_similar_chunks(
                query_vector=query_vector,
                top_k=top_k,
                threshold=threshold
            )
            print(f"ğŸ“„ æ£€ç´¢åˆ° {len(similar_chunks)} ä¸ªç›¸ä¼¼æ–‡æ¡£å—")
            
            # æ‰“å°æ¯ä¸ªå—çš„è¯¦ç»†ä¿¡æ¯
            for i, chunk in enumerate(similar_chunks):
                similarity = chunk.metadata.get('similarity', 0.0)
                print(f"  å— {i+1}: ç›¸ä¼¼åº¦={similarity:.4f}, å†…å®¹é¢„è§ˆ='{chunk.content[:50]}...'")
            
            # 3. æ„å»ºä¸Šä¸‹æ–‡
            context_texts = []
            sources = []
            
            for chunk in similar_chunks:
                context_texts.append(chunk.content)
                
                # æ„å»ºæ¥æºä¿¡æ¯
                source_info = {
                    'chunk_id': chunk.id,
                    'content_preview': chunk.content[:100] + '...' if len(chunk.content) > 100 else chunk.content,
                    'similarity': chunk.metadata.get('similarity', 0.0)
                }
                
                # æ·»åŠ æ–‡æ¡£ä¿¡æ¯
                if 'document_id' in chunk.metadata:
                    source_info['document_id'] = chunk.metadata['document_id']
                if 'document_title' in chunk.metadata:
                    source_info['document_title'] = chunk.metadata['document_title']
                
                sources.append(source_info)
            
            # 4. ç”Ÿæˆç­”æ¡ˆ
            if context_texts:
                print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
                answer = await self.llm_service.generate_answer(
                    query=request.query,
                    context=context_texts
                )
                print(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ: '{answer[:100]}...'")
            else:
                answer = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ã€‚"
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹")
            
            # 5. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
            confidence = 0.0
            if sources:
                avg_similarity = sum(s['similarity'] for s in sources) / len(sources)
                confidence = min(avg_similarity, 1.0)
                print(f"ğŸ“ˆ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}, ç½®ä¿¡åº¦: {confidence:.4f}")
            
            processing_time = time.time() - start_time
            print(f"â±ï¸ å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.4f}ç§’")
            
            return QueryResponse(
                query=request.query,
                answer=answer,
                sources=sources if request.include_metadata else [],
                confidence=confidence,
                processing_time=processing_time,
                total_chunks_retrieved=len(similar_chunks)
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"âŒ å›é€€æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return QueryResponse(
                query=request.query,
                answer=f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
                sources=[],
                confidence=0.0,
                processing_time=processing_time,
                total_chunks_retrieved=0
            )
    
    async def search_documents(self, query: str, top_k: int = 10, threshold: float = 0.5) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£ï¼ˆè¿”å›æ–‡æ¡£çº§åˆ«çš„ç»“æœï¼‰"""
        try:
            print(f"ğŸ” æœç´¢æ–‡æ¡£: '{query}', top_k={top_k}, threshold={threshold}")
            
            # ä¼˜å…ˆä½¿ç”¨LangChainå­˜å‚¨ç³»ç»Ÿ
            try:
                print("ğŸ¤– ä½¿ç”¨LangChainå­˜å‚¨ç³»ç»Ÿæœç´¢...")
                results = await self.langchain_service.search_documents(
                    query=query,
                    top_k=top_k,
                    threshold=threshold
                )
                print(f"âœ… LangChainæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                return results
                
            except Exception as e:
                print(f"âŒ LangChainæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰å­˜å‚¨ç³»ç»Ÿ: {e}")
                
                # å›é€€åˆ°åŸæœ‰å­˜å‚¨ç³»ç»Ÿ
                print("ğŸ”„ ä½¿ç”¨åŸæœ‰å­˜å‚¨ç³»ç»Ÿæœç´¢...")
                
                # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
                query_vector = self.embedding_service.encode_single_text(query)
                
                # æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£å—
                similar_chunks = await self.storage.search_similar_chunks(
                    query_vector=query_vector,
                    top_k=top_k * 2,  # è·å–æ›´å¤šå—ä»¥ä¾¿å»é‡
                    threshold=threshold
                )
                
                # æŒ‰æ–‡æ¡£åˆ†ç»„å¹¶è®¡ç®—æ–‡æ¡£çº§åˆ«çš„ç›¸ä¼¼åº¦
                document_scores = {}
                
                for chunk in similar_chunks:
                    doc_id = chunk.metadata.get('document_id')
                    if doc_id:
                        if doc_id not in document_scores:
                            document_scores[doc_id] = {
                                'document_id': doc_id,
                                'document_title': chunk.metadata.get('document_title', 'Unknown'),
                                'chunks': [],
                                'max_similarity': 0.0,
                                'avg_similarity': 0.0
                            }
                        
                        similarity = chunk.metadata.get('similarity', 0.0)
                        document_scores[doc_id]['chunks'].append({
                            'chunk_id': chunk.id,
                            'content_preview': chunk.content[:150] + '...' if len(chunk.content) > 150 else chunk.content,
                            'similarity': similarity
                        })
                        
                        # æ›´æ–°æ–‡æ¡£çº§åˆ«çš„ç›¸ä¼¼åº¦
                        doc_scores = document_scores[doc_id]
                        doc_scores['max_similarity'] = max(doc_scores['max_similarity'], similarity)
                
                # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
                for doc_id, doc_info in document_scores.items():
                    similarities = [chunk['similarity'] for chunk in doc_info['chunks']]
                    doc_info['avg_similarity'] = sum(similarities) / len(similarities)
                    # åªä¿ç•™å‰3ä¸ªæœ€ç›¸å…³çš„å—
                    doc_info['chunks'] = sorted(doc_info['chunks'], key=lambda x: x['similarity'], reverse=True)[:3]
                
                # æŒ‰æœ€å¤§ç›¸ä¼¼åº¦æ’åº
                results = list(document_scores.values())
                results.sort(key=lambda x: x['max_similarity'], reverse=True)
                
                print(f"âœ… åŸæœ‰å­˜å‚¨ç³»ç»Ÿæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                return results[:top_k]
            
        except Exception as e:
            print(f"âŒ æœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            return []
    
    async def get_suggestions(self, query: str, max_suggestions: int = 5) -> List[str]:
        """è·å–æŸ¥è¯¢å»ºè®®"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å»ºè®®é€»è¾‘
            # ç›®å‰è¿”å›ä¸€äº›ç®€å•çš„å»ºè®®
            suggestions = []
            
            # åŸºäºæŸ¥è¯¢é•¿åº¦æä¾›å»ºè®®
            if len(query) < 3:
                suggestions.append("è¯·æä¾›æ›´è¯¦ç»†çš„æŸ¥è¯¢")
            elif len(query) > 100:
                suggestions.append("è¯·ç®€åŒ–æ‚¨çš„æŸ¥è¯¢")
            
            # æ·»åŠ ä¸€äº›é€šç”¨å»ºè®®
            common_suggestions = [
                "å°è¯•ä½¿ç”¨å…³é”®è¯è€Œä¸æ˜¯å®Œæ•´å¥å­",
                "æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®",
                "ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯"
            ]
            
            suggestions.extend(common_suggestions[:max_suggestions - len(suggestions)])
            
            return suggestions
            
        except Exception as e:
            print(f"è·å–æŸ¥è¯¢å»ºè®®å¤±è´¥: {e}")
            return []
    
    async def health_check(self) -> dict:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥LangChain RAGæœåŠ¡
            langchain_health = await self.langchain_service.health_check()
            
            # æ£€æŸ¥åŸæœ‰æœåŠ¡
            storage_health = await self.storage.health_check()
            embedding_health = self.embedding_service.get_model_info()
            llm_health = await self.llm_service.health_check()
            
            return {
                'status': 'healthy' if langchain_health.get('status') == 'healthy' else 'unhealthy',
                'langchain_rag': langchain_health,
                'legacy_services': {
                    'storage': {
                        'backend': type(self.storage).__name__,
                        'healthy': storage_health
                    },
                    'embedding': embedding_health,
                    'llm': llm_health
                }
            }
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            } 
"""
LangChain RAGæœåŠ¡
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.schema import Document
from pydantic import Field
from app.config import settings
import aiohttp
import json


class DeepSeekLLM(LLM):
    """DeepSeek LLMåŒ…è£…å™¨"""
    
    api_key: str = Field(description="DeepSeek APIå¯†é’¥")
    api_url: str = Field(description="DeepSeek API URL")
    model: str = Field(description="DeepSeekæ¨¡åž‹åç§°")
    
    def __init__(self, api_key: str, api_url: str, model: str):
        super().__init__(api_key=api_key, api_url=api_url, model=model)
    
    @property
    def _llm_type(self) -> str:
        return "deepseek"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """åŒæ­¥è°ƒç”¨DeepSeek API"""
        return asyncio.run(self._acall(prompt, stop, run_manager, **kwargs))
    
    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """å¼‚æ­¥è°ƒç”¨DeepSeek API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": kwargs.get("max_tokens", 1000),
            "temperature": kwargs.get("temperature", 0.7),
            "stream": False
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    error_text = await response.text()
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status} - {error_text}")


class LangChainRAGService:
    """LangChain RAGæœåŠ¡ - å•ä¾‹æ¨¡å¼"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LangChainRAGService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # ç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡
        if not self._initialized:
            print(f"[DEBUG] LangChainæœåŠ¡åˆå§‹åŒ–ï¼Œé…ç½®çš„æ¨¡åž‹: {settings.embedding_model}")
            self.embeddings = SentenceTransformerEmbeddings(
                model_name=settings.embedding_model,
                model_kwargs={'device': settings.embedding_device}
            )
            
            # æ‰“å°å®žé™…åŠ è½½çš„æ¨¡åž‹ä¿¡æ¯
            try:
                # å°è¯•èŽ·å–åº•å±‚æ¨¡åž‹ä¿¡æ¯
                if hasattr(self.embeddings, 'client') and hasattr(self.embeddings.client, 'get_sentence_embedding_dimension'):
                    dim = self.embeddings.client.get_sentence_embedding_dimension()
                    print(f"[DEBUG] LangChain embeddingså®žé™…ç»´åº¦: {dim}")
                else:
                    print("[DEBUG] æ— æ³•èŽ·å–LangChain embeddingsç»´åº¦ä¿¡æ¯")
            except Exception as e:
                print(f"[DEBUG] èŽ·å–LangChain embeddingsä¿¡æ¯å¤±è´¥: {e}")
            
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap,
                length_function=len,
            )
            
            self.llm = DeepSeekLLM(
                api_key=settings.deepseek_api_key,
                api_url=settings.deepseek_api_url,
                model=settings.deepseek_model
            )
            
            self.vector_store = None
            self.qa_chain = None
            self._initialize_vector_store()
            
            self._initialized = True
    
    def _initialize_vector_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
        os.makedirs(vector_store_path, exist_ok=True)
        
        # å°è¯•åŠ è½½çŽ°æœ‰çš„å‘é‡å­˜å‚¨
        if os.path.exists(f"{vector_store_path}/index.faiss"):
            try:
                self.vector_store = FAISS.load_local(
                    vector_store_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True  # å…è®¸åŠ è½½æœ¬åœ°æ–‡ä»¶
                )
                print("âœ… æˆåŠŸåŠ è½½çŽ°æœ‰å‘é‡å­˜å‚¨")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
                self.vector_store = None
        
        # å¦‚æžœæ²¡æœ‰çŽ°æœ‰å­˜å‚¨ï¼Œåˆ›å»ºæ–°çš„ç©ºå‘é‡å­˜å‚¨
        if self.vector_store is None:
            # åˆ›å»ºä¸€ä¸ªçœŸæ­£çš„ç©ºå‘é‡å­˜å‚¨
            # ä½¿ç”¨ä¸€ä¸ªä¸´æ—¶æ–‡æ¡£åˆ›å»ºå­˜å‚¨ï¼Œç„¶åŽç«‹å³æ¸…ç©º
            temp_doc = Document(page_content="temp", metadata={})
            self.vector_store = FAISS.from_documents([temp_doc], self.embeddings)
            
            # æ¸…ç©ºå­˜å‚¨ï¼Œç¡®ä¿æ˜¯çœŸæ­£çš„ç©ºå­˜å‚¨
            try:
                # èŽ·å–æ‰€æœ‰æ–‡æ¡£IDå¹¶åˆ é™¤
                doc_ids = list(self.vector_store.index_to_docstore_id.values())
                if doc_ids:
                    self.vector_store.delete(doc_ids)
                print("âœ… åˆ›å»ºæ–°çš„ç©ºå‘é‡å­˜å‚¨")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç©ºå‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
                # å¦‚æžœæ¸…ç©ºå¤±è´¥ï¼Œé‡æ–°åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºå­˜å‚¨
                self.vector_store = FAISS.from_documents([], self.embeddings)
                print("âœ… é‡æ–°åˆ›å»ºç©ºå‘é‡å­˜å‚¨")
    
    def add_documents(self, documents: List[Dict[str, Any]]) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            print(f"ðŸ” å¼€å§‹æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°LangChainå­˜å‚¨")
            
            # è½¬æ¢ä¸ºLangChain Documentæ ¼å¼
            langchain_docs = []
            for doc in documents:
                print(f"  å¤„ç†æ–‡æ¡£: {doc.get('title', 'Unknown')} (ID: {doc['id']})")
                langchain_doc = Document(
                    page_content=doc["content"],
                    metadata={
                        "document_id": doc["id"],
                        "title": doc["title"],
                        "created_at": doc.get("created_at", ""),
                        "file_type": doc.get("file_type", "")
                    }
                )
                langchain_docs.append(langchain_doc)
            
            print(f"âœ… è½¬æ¢ä¸ºLangChainæ ¼å¼å®Œæˆï¼Œå…± {len(langchain_docs)} ä¸ªæ–‡æ¡£")
            
            # åˆ†å—
            print("ðŸ“ å¼€å§‹æ–‡æ¡£åˆ†å—...")
            split_docs = self.text_splitter.split_documents(langchain_docs)
            print(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(split_docs)} ä¸ªæ–‡æ¡£å—")
            
            # æ‰“å°åˆ†å—è¯¦æƒ…
            for i, doc in enumerate(split_docs):
                print(f"  å— {i+1}: é•¿åº¦={len(doc.page_content)}, å…ƒæ•°æ®={doc.metadata}")
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            print("ðŸ’¾ å¼€å§‹æ·»åŠ åˆ°å‘é‡å­˜å‚¨...")
            if self.vector_store is None:
                print("âš ï¸ å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œåˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨")
                self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
            else:
                print(f"ðŸ“Š å½“å‰å‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
                self.vector_store.add_documents(split_docs)
                print(f"ðŸ“Š æ·»åŠ åŽå‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            
            # ä¿å­˜å‘é‡å­˜å‚¨
            print("ðŸ’¾ ä¿å­˜å‘é‡å­˜å‚¨åˆ°æœ¬åœ°...")
            vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
            self.vector_store.save_local(vector_store_path)
            print(f"âœ… å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {vector_store_path}")
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(split_docs)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡å­˜å‚¨")
            return True
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def query(self, query: str, top_k: int = 5, threshold: float = 0.3) -> Dict[str, Any]:
        """æŸ¥è¯¢é—®ç­”"""
        try:
            if self.vector_store is None:
                print(f"âŒ å‘é‡å­˜å‚¨ä¸ºç©º")
                return {
                    "answer": "æŠ±æ­‰ï¼Œå‘é‡å­˜å‚¨æœªåˆå§‹åŒ–",
                    "sources": [],
                    "confidence": 0.0
                }
            
            print(f"ðŸ” LangChainæŸ¥è¯¢: '{query}', top_k={top_k}, threshold={threshold}")
            
            # åˆ›å»ºæ£€ç´¢å™¨ - æš‚æ—¶ä¸ä½¿ç”¨score_thresholdï¼Œè®©æ‰€æœ‰ç»“æžœéƒ½è¿”å›ž
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": top_k * 2,  # èŽ·å–æ›´å¤šå—ä»¥ä¾¿è¿‡æ»¤
                    # æš‚æ—¶ä¸ä½¿ç”¨score_thresholdï¼Œè®©æ‰€æœ‰ç»“æžœéƒ½è¿”å›ž
                }
            )
            
            # åˆ›å»ºQAé“¾
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={
                    "prompt": self._get_qa_prompt()
                }
            )
            
            print("query is :", query)
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await qa_chain.ainvoke({"query": query})
            
            # å¤„ç†ç»“æžœ
            answer = result.get("result", "")
            print(f"[DEBUG] æŸ¥è¯¢ç»“æžœ: {answer}")
            source_docs = result.get("source_documents", [])
            print(f"[DEBUG] æ£€ç´¢åˆ° {len(source_docs)} ä¸ªæ–‡æ¡£å—")
            
            # æž„å»ºæ¥æºä¿¡æ¯
            sources = []
            for doc in source_docs:
                source_info = {
                    "content_preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content,
                    "metadata": doc.metadata
                }
                sources.append(source_info)
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": 0.8 if sources else 0.0  # ç®€åŒ–ç½®ä¿¡åº¦è®¡ç®—
            }
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "answer": f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }
    
    async def search_documents(self, query: str, top_k: int = 10, threshold: float = 0.1) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£ï¼ˆè¿”å›žæ–‡æ¡£çº§åˆ«çš„ç»“æžœï¼‰"""
        try:
            print(f"ðŸ” LangChainæœç´¢æ–‡æ¡£: '{query}', top_k={top_k}, threshold={threshold}")
            
            if self.vector_store is None:
                print("âŒ å‘é‡å­˜å‚¨ä¸ºç©º")
                return []
            
            print(f"ðŸ“Š å‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            
            # åˆ›å»ºæ£€ç´¢å™¨ï¼Œä¸ä½¿ç”¨score_thresholdï¼Œè®©æ‰€æœ‰ç»“æžœéƒ½è¿”å›ž
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": top_k * 2,  # èŽ·å–æ›´å¤šå—ä»¥ä¾¿åŽ»é‡
                    # æš‚æ—¶ä¸ä½¿ç”¨score_thresholdï¼Œè®©æ‰€æœ‰ç»“æžœéƒ½è¿”å›ž
                }
            )
            
            # æ‰§è¡Œæœç´¢
            print("ðŸ” æ‰§è¡Œå‘é‡æœç´¢...")
            docs_and_scores = await retriever.aget_relevant_documents(query)
            print(f"ðŸ“„ æœç´¢åˆ° {len(docs_and_scores)} ä¸ªæ–‡æ¡£å—")
            
            # æ‰“å°æ¯ä¸ªå—çš„è¯¦ç»†ä¿¡æ¯
            for i, doc in enumerate(docs_and_scores):
                print(f"  å— {i+1}: å†…å®¹='{doc.page_content[:50]}...', å…ƒæ•°æ®={doc.metadata}")
            
            # æŒ‰æ–‡æ¡£åˆ†ç»„å¹¶è®¡ç®—æ–‡æ¡£çº§åˆ«çš„ç›¸ä¼¼åº¦
            document_scores = {}
            
            for doc in docs_and_scores:
                doc_id = doc.metadata.get('document_id')
                if doc_id:
                    if doc_id not in document_scores:
                        document_scores[doc_id] = {
                            'document_id': doc_id,
                            'document_title': doc.metadata.get('title', 'Unknown'),
                            'chunks': [],
                            'max_similarity': 0.0,
                            'avg_similarity': 0.0
                        }
                    
                    # ä½¿ç”¨è¾ƒé«˜çš„é»˜è®¤ç›¸ä¼¼åº¦ï¼Œå› ä¸ºLangChainå·²ç»é€šè¿‡å‘é‡æœç´¢è¿‡æ»¤äº†
                    similarity = 0.9  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                    document_scores[doc_id]['chunks'].append({
                        'chunk_id': doc_id,  # ä½¿ç”¨æ–‡æ¡£IDä½œä¸ºå—ID
                        'content_preview': doc.page_content[:150] + '...' if len(doc.page_content) > 150 else doc.page_content,
                        'similarity': similarity
                    })
                    
                    # æ›´æ–°æ–‡æ¡£çº§åˆ«çš„ç›¸ä¼¼åº¦
                    doc_scores = document_scores[doc_id]
                    doc_scores['max_similarity'] = max(doc_scores['max_similarity'], similarity)
            
            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
            for doc_id, doc_info in document_scores.items():
                similarities = [chunk['similarity'] for chunk in doc_info['chunks']]
                doc_info['avg_similarity'] = sum(similarities) / len(similarities)
                # åªä¿ç•™å‰3ä¸ªæœ€ç›¸å…³çš„å—
                doc_info['chunks'] = sorted(doc_info['chunks'], key=lambda x: x['similarity'], reverse=True)[:3]
            
            # æŒ‰æœ€å¤§ç›¸ä¼¼åº¦æŽ’åº
            results = list(document_scores.values())
            results.sort(key=lambda x: x['max_similarity'], reverse=True)
            
            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
            filtered_results = [r for r in results if r['max_similarity'] >= threshold]
            print(f"âœ… è¿‡æ»¤åŽç»“æžœ: {len(filtered_results)} ä¸ªæ–‡æ¡£ (é˜ˆå€¼: {threshold})")
            
            return filtered_results[:top_k]
            
        except Exception as e:
            print(f"âŒ LangChainæœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def delete_document(self, document_id: str) -> bool:
        """ä»Žå‘é‡å­˜å‚¨ä¸­åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡"""
        try:
            print(f"ðŸ—‘ï¸ ä»ŽLangChainå­˜å‚¨åˆ é™¤æ–‡æ¡£: {document_id}")
            
            if self.vector_store is None:
                print("âŒ å‘é‡å­˜å‚¨ä¸ºç©º")
                return False
            
            print(f"ðŸ“Š åˆ é™¤å‰å‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            
            # æ‰¾åˆ°è¦åˆ é™¤çš„æ–‡æ¡£å—ID
            doc_ids_to_delete = []
            for idx, doc_id in self.vector_store.index_to_docstore_id.items():
                # èŽ·å–æ–‡æ¡£çš„å…ƒæ•°æ®
                doc = self.vector_store.docstore._dict.get(doc_id)
                if doc and doc.metadata.get('document_id') == document_id:
                    doc_ids_to_delete.append(doc_id)
            
            if not doc_ids_to_delete:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ {document_id} çš„å‘é‡æ•°æ®")
                return True  # è®¤ä¸ºåˆ é™¤æˆåŠŸï¼Œå› ä¸ºæœ¬æ¥å°±æ²¡æœ‰
            
            print(f"ðŸ” æ‰¾åˆ° {len(doc_ids_to_delete)} ä¸ªç›¸å…³æ–‡æ¡£å—")
            
            # åˆ é™¤æ–‡æ¡£å—
            self.vector_store.delete(doc_ids_to_delete)
            
            # ä¿å­˜æ›´æ–°åŽçš„å‘é‡å­˜å‚¨
            vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
            self.vector_store.save_local(vector_store_path)
            
            print(f"ðŸ“Š åˆ é™¤åŽå‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            print(f"âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} çš„å‘é‡æ•°æ®")
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_qa_prompt(self):
        """èŽ·å–QAæç¤ºè¯æ¨¡æ¿"""
        from langchain_core.prompts import PromptTemplate
        
        template = """åŸºäºŽä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æžœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜Žæ— æ³•ä»Žæä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„å›žç­”ï¼š"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–ç»Ÿè®¡ä¿¡æ¯"""
        if self.vector_store is None:
            return {"total_documents": 0, "total_chunks": 0}
        
        try:
            # èŽ·å–å‘é‡å­˜å‚¨ä¿¡æ¯
            index = self.vector_store.index
            total_chunks = index.ntotal if hasattr(index, 'ntotal') else 0
            
            return {
                "total_documents": 0,  # éœ€è¦ä»Žå…ƒæ•°æ®ç»Ÿè®¡
                "total_chunks": total_chunks
            }
        except Exception as e:
            print(f"èŽ·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_documents": 0, "total_chunks": 0}
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•å‘é‡å­˜å‚¨
            vector_store_ok = self.vector_store is not None
            
            # æµ‹è¯•LLM
            test_result = await self.llm._acall("æµ‹è¯•", max_tokens=5)
            llm_ok = len(test_result) > 0
            
            return {
                "status": "healthy" if vector_store_ok and llm_ok else "unhealthy",
                "vector_store": vector_store_ok,
                "llm": llm_ok,
                "stats": self.get_stats()
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            } 
"""
LangChain RAGæœåŠ¡
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.schema import Document
from pydantic import Field
from app.config import settings
import aiohttp
import json


class DeepSeekLLM(LLM):
    """DeepSeek LLMåŒ…è£…å™¨"""
    
    api_key: str = Field(description="DeepSeek APIå¯†é’¥")
    api_url: str = Field(description="DeepSeek API URL")
    model: str = Field(description="DeepSeekæ¨¡å‹åç§°")
    
    def __init__(self, api_key: str, api_url: str, model: str):
        super().__init__(api_key=api_key, api_url=api_url, model=model)
    
    @property
    def _llm_type(self) -> str:
        return "deepseek"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """åŒæ­¥è°ƒç”¨DeepSeek API"""
        return asyncio.run(self._acall(prompt, stop, run_manager, **kwargs))
    
    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """å¼‚æ­¥è°ƒç”¨DeepSeek API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": kwargs.get("max_tokens", 1000),
            "temperature": kwargs.get("temperature", 0.7),
            "stream": False
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    error_text = await response.text()
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status} - {error_text}")


class LangChainRAGService:
    """LangChain RAGæœåŠ¡ - å•ä¾‹æ¨¡å¼"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LangChainRAGService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # ç¡®ä¿åªåˆå§‹åŒ–ä¸€æ¬¡
        if not self._initialized:
            print(f"[DEBUG] LangChainæœåŠ¡åˆå§‹åŒ–ï¼Œé…ç½®çš„æ¨¡å‹: {settings.embedding_model}")
            self.embeddings = SentenceTransformerEmbeddings(
                model_name=settings.embedding_model,
                model_kwargs={'device': settings.embedding_device}
            )
            
            # æ‰“å°å®é™…åŠ è½½çš„æ¨¡å‹ä¿¡æ¯
            try:
                # å°è¯•è·å–åº•å±‚æ¨¡å‹ä¿¡æ¯
                if hasattr(self.embeddings, 'client') and hasattr(self.embeddings.client, 'get_sentence_embedding_dimension'):
                    dim = self.embeddings.client.get_sentence_embedding_dimension()
                    print(f"[DEBUG] LangChain embeddingså®é™…ç»´åº¦: {dim}")
                else:
                    print("[DEBUG] æ— æ³•è·å–LangChain embeddingsç»´åº¦ä¿¡æ¯")
            except Exception as e:
                print(f"[DEBUG] è·å–LangChain embeddingsä¿¡æ¯å¤±è´¥: {e}")
            
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap,
                length_function=len,
            )
            
            self.llm = DeepSeekLLM(
                api_key=settings.deepseek_api_key,
                api_url=settings.deepseek_api_url,
                model=settings.deepseek_model
            )
            
            self.vector_store = None
            self.qa_chain = None
            self._initialize_vector_store()
            
            self._initialized = True
    
    def _initialize_vector_store(self):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨"""
        vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
        os.makedirs(vector_store_path, exist_ok=True)
        
        # å°è¯•åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨
        if os.path.exists(f"{vector_store_path}/index.faiss"):
            try:
                self.vector_store = FAISS.load_local(
                    vector_store_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True  # å…è®¸åŠ è½½æœ¬åœ°æ–‡ä»¶
                )
                print("âœ… æˆåŠŸåŠ è½½ç°æœ‰å‘é‡å­˜å‚¨")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
                self.vector_store = None
        
        # å¦‚æœæ²¡æœ‰ç°æœ‰å­˜å‚¨ï¼Œåˆ›å»ºæ–°çš„ç©ºå‘é‡å­˜å‚¨
        if self.vector_store is None:
            # åˆ›å»ºä¸€ä¸ªçœŸæ­£çš„ç©ºå‘é‡å­˜å‚¨
            # ä½¿ç”¨ä¸€ä¸ªä¸´æ—¶æ–‡æ¡£åˆ›å»ºå­˜å‚¨ï¼Œç„¶åç«‹å³æ¸…ç©º
            temp_doc = Document(page_content="temp", metadata={})
            self.vector_store = FAISS.from_documents([temp_doc], self.embeddings)
            
            # æ¸…ç©ºå­˜å‚¨ï¼Œç¡®ä¿æ˜¯çœŸæ­£çš„ç©ºå­˜å‚¨
            try:
                # è·å–æ‰€æœ‰æ–‡æ¡£IDå¹¶åˆ é™¤
                doc_ids = list(self.vector_store.index_to_docstore_id.values())
                if doc_ids:
                    self.vector_store.delete(doc_ids)
                print("âœ… åˆ›å»ºæ–°çš„ç©ºå‘é‡å­˜å‚¨")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç©ºå‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
                # å¦‚æœæ¸…ç©ºå¤±è´¥ï¼Œé‡æ–°åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºå­˜å‚¨
                self.vector_store = FAISS.from_documents([], self.embeddings)
                print("âœ… é‡æ–°åˆ›å»ºç©ºå‘é‡å­˜å‚¨")
    
    def add_documents(self, documents: List[Dict[str, Any]]) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            print(f"ğŸ” å¼€å§‹æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°LangChainå­˜å‚¨")
            
            # è½¬æ¢ä¸ºLangChain Documentæ ¼å¼
            langchain_docs = []
            for doc in documents:
                print(f"  å¤„ç†æ–‡æ¡£: {doc.get('title', 'Unknown')} (ID: {doc['id']})")
                langchain_doc = Document(
                    page_content=doc["content"],
                    metadata={
                        "document_id": doc["id"],
                        "title": doc["title"],
                        "created_at": doc.get("created_at", ""),
                        "file_type": doc.get("file_type", "")
                    }
                )
                langchain_docs.append(langchain_doc)
            
            print(f"âœ… è½¬æ¢ä¸ºLangChainæ ¼å¼å®Œæˆï¼Œå…± {len(langchain_docs)} ä¸ªæ–‡æ¡£")
            
            # åˆ†å—
            print("ğŸ“ å¼€å§‹æ–‡æ¡£åˆ†å—...")
            split_docs = self.text_splitter.split_documents(langchain_docs)
            print(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(split_docs)} ä¸ªæ–‡æ¡£å—")
            
            # æ‰“å°åˆ†å—è¯¦æƒ…
            for i, doc in enumerate(split_docs):
                print(f"  å— {i+1}: é•¿åº¦={len(doc.page_content)}, å…ƒæ•°æ®={doc.metadata}")
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            print("ğŸ’¾ å¼€å§‹æ·»åŠ åˆ°å‘é‡å­˜å‚¨...")
            if self.vector_store is None:
                print("âš ï¸ å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œåˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨")
                self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
            else:
                print(f"ğŸ“Š å½“å‰å‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
                self.vector_store.add_documents(split_docs)
                print(f"ğŸ“Š æ·»åŠ åå‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            
            # ä¿å­˜å‘é‡å­˜å‚¨
            print("ğŸ’¾ ä¿å­˜å‘é‡å­˜å‚¨åˆ°æœ¬åœ°...")
            vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
            self.vector_store.save_local(vector_store_path)
            print(f"âœ… å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {vector_store_path}")
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(split_docs)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡å­˜å‚¨")
            return True
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
            return {
                "answer": "æŠ±æ­‰ï¼ŒæŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚",
                "sources": [],
                "confidence": 0.0
            }               
            
    
    async def search_documents(self, query: str, top_k: int = 10, threshold: float = 0.1) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£ï¼ˆè¿”å›æ–‡æ¡£çº§åˆ«çš„ç»“æœï¼‰"""
        try:
            print(f"ğŸ” LangChainæœç´¢æ–‡æ¡£: '{query}', top_k={top_k}, threshold={threshold}")
            
            if self.vector_store is None:
                print("âŒ å‘é‡å­˜å‚¨ä¸ºç©º")
                return []
            
            print(f"ğŸ“Š å‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            
            # åˆ›å»ºæ£€ç´¢å™¨ï¼Œä¸ä½¿ç”¨score_thresholdï¼Œè®©æ‰€æœ‰ç»“æœéƒ½è¿”å›
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={
                    "k": top_k * 2,  # è·å–æ›´å¤šå—ä»¥ä¾¿å»é‡
                    # æš‚æ—¶ä¸ä½¿ç”¨score_thresholdï¼Œè®©æ‰€æœ‰ç»“æœéƒ½è¿”å›
                }
            )
            
            # æ‰§è¡Œæœç´¢
            print("ğŸ” æ‰§è¡Œå‘é‡æœç´¢...")
            docs_and_scores = await retriever.aget_relevant_documents(query)
            print(f"ğŸ“„ æœç´¢åˆ° {len(docs_and_scores)} ä¸ªæ–‡æ¡£å—")
            
            # æ‰“å°æ¯ä¸ªå—çš„è¯¦ç»†ä¿¡æ¯
            for i, doc in enumerate(docs_and_scores):
                print(f"  å— {i+1}: å†…å®¹='{doc.page_content[:50]}...', å…ƒæ•°æ®={doc.metadata}")
            
            # æŒ‰æ–‡æ¡£åˆ†ç»„å¹¶è®¡ç®—æ–‡æ¡£çº§åˆ«çš„ç›¸ä¼¼åº¦
            document_scores = {}
            
            for doc in docs_and_scores:
                doc_id = doc.metadata.get('document_id')
                if doc_id:
                    if doc_id not in document_scores:
                        document_scores[doc_id] = {
                            'document_id': doc_id,
                            'document_title': doc.metadata.get('title', 'Unknown'),
                            'chunks': [],
                            'max_similarity': 0.0,
                            'avg_similarity': 0.0
                        }
                    
                    # ä½¿ç”¨è¾ƒé«˜çš„é»˜è®¤ç›¸ä¼¼åº¦ï¼Œå› ä¸ºLangChainå·²ç»é€šè¿‡å‘é‡æœç´¢è¿‡æ»¤äº†
                    similarity = 0.9  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                    document_scores[doc_id]['chunks'].append({
                        'chunk_id': doc_id,  # ä½¿ç”¨æ–‡æ¡£IDä½œä¸ºå—ID
                        'content_preview': doc.page_content[:150] + '...' if len(doc.page_content) > 150 else doc.page_content,
                        'similarity': similarity
                    })
                    
                    # æ›´æ–°æ–‡æ¡£çº§åˆ«çš„ç›¸ä¼¼åº¦
                    doc_scores = document_scores[doc_id]
                    doc_scores['max_similarity'] = max(doc_scores['max_similarity'], similarity)
            
            # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
            for doc_id, doc_info in document_scores.items():
                similarities = [chunk['similarity'] for chunk in doc_info['chunks']]
                doc_info['avg_similarity'] = sum(similarities) / len(similarities)
                # åªä¿ç•™å‰3ä¸ªæœ€ç›¸å…³çš„å—
                doc_info['chunks'] = sorted(doc_info['chunks'], key=lambda x: x['similarity'], reverse=True)[:3]
            
            # æŒ‰æœ€å¤§ç›¸ä¼¼åº¦æ’åº
            results = list(document_scores.values())
            results.sort(key=lambda x: x['max_similarity'], reverse=True)
            
            # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
            filtered_results = [r for r in results if r['max_similarity'] >= threshold]
            print(f"âœ… è¿‡æ»¤åç»“æœ: {len(filtered_results)} ä¸ªæ–‡æ¡£ (é˜ˆå€¼: {threshold})")
            
            return filtered_results[:top_k]
            
        except Exception as e:
            print(f"âŒ LangChainæœç´¢æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def delete_document(self, document_id: str) -> bool:
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰å‘é‡"""
        try:
            print(f"ğŸ—‘ï¸ ä»LangChainå­˜å‚¨åˆ é™¤æ–‡æ¡£: {document_id}")
            
            if self.vector_store is None:
                print("âŒ å‘é‡å­˜å‚¨ä¸ºç©º")
                return False
            
            print(f"ğŸ“Š åˆ é™¤å‰å‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            
            # æ‰¾åˆ°è¦åˆ é™¤çš„æ–‡æ¡£å—ID
            doc_ids_to_delete = []
            for idx, doc_id in self.vector_store.index_to_docstore_id.items():
                # è·å–æ–‡æ¡£çš„å…ƒæ•°æ®
                doc = self.vector_store.docstore._dict.get(doc_id)
                if doc and doc.metadata.get('document_id') == document_id:
                    doc_ids_to_delete.append(doc_id)
            
            if not doc_ids_to_delete:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£ {document_id} çš„å‘é‡æ•°æ®")
                return True  # è®¤ä¸ºåˆ é™¤æˆåŠŸï¼Œå› ä¸ºæœ¬æ¥å°±æ²¡æœ‰
            
            print(f"ğŸ” æ‰¾åˆ° {len(doc_ids_to_delete)} ä¸ªç›¸å…³æ–‡æ¡£å—")
            
            # åˆ é™¤æ–‡æ¡£å—
            self.vector_store.delete(doc_ids_to_delete)
            
            # ä¿å­˜æ›´æ–°åçš„å‘é‡å­˜å‚¨
            vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
            self.vector_store.save_local(vector_store_path)
            
            print(f"ğŸ“Š åˆ é™¤åå‘é‡å­˜å‚¨çŠ¶æ€: {len(self.vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£")
            print(f"âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} çš„å‘é‡æ•°æ®")
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_qa_prompt(self):
        """è·å–QAæç¤ºè¯æ¨¡æ¿"""
        from langchain_core.prompts import PromptTemplate
        
        template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„å›ç­”ï¼š"""


        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.vector_store is None:
                return {
                    "total_chunks": 0,
                    "vector_store_ok": False,
                    "llm_ok": False
                }
            
            # è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡
            total_chunks = len(self.vector_store.index_to_docstore_id) if self.vector_store else 0
            
            # æµ‹è¯•å‘é‡å­˜å‚¨
            vector_store_ok = self.vector_store is not None
            
            # æµ‹è¯•LLMï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
            llm_ok = self.llm is not None
            
            return {
                "total_chunks": total_chunks,
                "vector_store_ok": vector_store_ok,
                "llm_ok": llm_ok
            }
            
        except Exception as e:
            print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "total_chunks": 0,
                "vector_store_ok": False,
                "llm_ok": False,
                "error": str(e)
            }
    async def query(self, query: str, top_k: int = 5, threshold: float = 0.3) -> Dict[str, Any]:
        """æŸ¥è¯¢é—®ç­”"""
        import numpy as np
        try:
            if self.vector_store is None:
                print(f"âŒ å‘é‡å­˜å‚¨ä¸ºç©º")
                return {
                    "answer": "å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–",
                    "sources": [],
                    "confidence": 0.0
                }
            print(f"ğŸ” LangChainæŸ¥è¯¢: '{query}', top_k={top_k}, threshold={threshold}")
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": top_k}
            )
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": self._get_qa_prompt()}
            )
            print("Push to llm.... Query is :", query)
            result = await qa_chain.ainvoke({"query": query})
            answer = result.get("result", "")
            source_docs = result.get("source_documents", [])
            sources = [{
                "content_preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content,
                "metadata": doc.metadata
            } for doc in source_docs]
            # è®¡ç®—çœŸå®çš„ç›¸ä¼¼åº¦
            total_similarity = 0.0
            for doc in source_docs:
                query_embedding = self.embeddings.embed_query(query)
                doc_embedding = self.embeddings.embed_query(doc.page_content)
                similarity = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))
                total_similarity += similarity
            return {
                "answer": answer,
                "sources": sources,
                "confidence": total_similarity / len(source_docs) if source_docs else 0.0
            }
        except Exception as e:
            print(f"âŒ LangChainæŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "answer": "æŠ±æ­‰ï¼ŒæŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚",
                "sources": [],
                "confidence": 0.0
            }







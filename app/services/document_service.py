import uuid
import time
from typing import List, Dict, Any, Optional
from app.models.document import Document, DocumentChunk, DocumentUploadRequest, DocumentListResponse
from app.models.storage import StorageBackend
from app.utils.text_processor import TextProcessor
from app.utils.embedding_service import EmbeddingService
from app.services.storage_factory import StorageFactory
from app.services.langchain_service import LangChainRAGService
from app.config import settings

class DocumentService:
    """æ–‡æ¡£æœåŠ¡ - é›†æˆLangChain"""
    
    def __init__(self):
        self.storage = StorageFactory.create_storage()
        self.text_processor = TextProcessor()
        self.embedding_service = EmbeddingService()
        self.langchain_service = LangChainRAGService()
    
    async def upload_document(self, request: DocumentUploadRequest) -> Document:
        """ä¸Šä¼ æ–‡æ¡£ - åŒæ—¶æ›´æ–°åŸæœ‰å­˜å‚¨å’ŒLangChainå­˜å‚¨"""
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            document_id = str(uuid.uuid4())
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = Document(
                id=document_id,
                title=request.title,
                content=request.content,
                file_type=request.file_type,
                file_size=len(request.content.encode('utf-8')),
                created_at=time.time(),
                updated_at=time.time(),
                metadata=request.metadata or {}
            )
            
            # 1. æ›´æ–°åŸæœ‰å­˜å‚¨ç³»ç»Ÿ
            # print("ğŸ“ æ›´æ–°åŸæœ‰å­˜å‚¨ç³»ç»Ÿ...")
            # success = await self.storage.save_document(document)
            # if not success:
            #     raise Exception("ä¿å­˜åˆ°åŸæœ‰å­˜å‚¨å¤±è´¥")
            
            # 2. æ›´æ–°LangChainå­˜å‚¨ç³»ç»Ÿ
            print("ğŸ¤– æ›´æ–°LangChainå­˜å‚¨ç³»ç»Ÿ...")
            langchain_docs = [{
                "id": document.id,
                "title": document.title,
                "content": document.content,
                "created_at": document.created_at,
                "file_type": document.file_type
            }]
            
            langchain_success = self.langchain_service.add_documents(langchain_docs)
            if not langchain_success:
                print("âš ï¸ è­¦å‘Šï¼šLangChainå­˜å‚¨æ›´æ–°å¤±è´¥ï¼Œä½†åŸæœ‰å­˜å‚¨å·²æˆåŠŸ")
            
            print(f"âœ… æ–‡æ¡£ä¸Šä¼ å®Œæˆ: {document.title}")
            return document
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
            raise
    
    async def upload_text_document(self, title: str, content: str, metadata: Optional[Dict[str, Any]] = None) -> Document:
        """ä¸Šä¼ æ–‡æœ¬æ–‡æ¡£"""
        request = DocumentUploadRequest(
            title=title,
            content=content,
            file_type="text",
            metadata=metadata or {}
        )
        return await self.upload_document(request)
    
    async def get_document(self, document_id: str) -> Optional[Document]:
        """è·å–æ–‡æ¡£"""
        try:
            return await self.storage.get_document(document_id)
        except Exception as e:
            print(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
            return None
    async def list_documents(self, page: int = 1, page_size: int = 10) -> DocumentListResponse:
        """ä»LangChainå­˜å‚¨è·å–æ–‡æ¡£åˆ—è¡¨"""
        try:
            print(f"ğŸ“š ä»LangChainå­˜å‚¨è·å–æ–‡æ¡£åˆ—è¡¨: page={page}, page_size={page_size}")
            
            # ä»LangChainå­˜å‚¨è·å–æ–‡æ¡£ä¿¡æ¯
            vector_store = self.langchain_service.vector_store
            if not vector_store:
                print("âŒ LangChainå‘é‡å­˜å‚¨ä¸ºç©º")
    
            
            print(f"ğŸ“Š LangChainå­˜å‚¨çŠ¶æ€: {len(vector_store.index_to_docstore_id)} ä¸ªæ–‡æ¡£å—")
            
            # ä»å‘é‡å­˜å‚¨ä¸­æå–æ–‡æ¡£ä¿¡æ¯
            documents = []
            doc_ids = set()
            
            for idx, doc_id in vector_store.index_to_docstore_id.items():
                doc = vector_store.docstore._dict.get(doc_id)
                if doc:
                    document_id = doc.metadata.get("document_id")
                    if document_id and document_id not in doc_ids:
                        doc_ids.add(document_id)
                        
                        # è®¡ç®—è¯¥æ–‡æ¡£çš„å—æ•°
                        chunk_count = sum(1 for i, stored_doc_id in vector_store.index_to_docstore_id.items()
                                        if vector_store.docstore._dict.get(stored_doc_id, {}).metadata.get("document_id") == document_id)
                        
                        documents.append({
                            "id": document_id,
                            "title": doc.metadata.get("title", "Unknown"),
                            "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                            "file_type": doc.metadata.get("file_type", "text"),
                            "file_size": len(doc.page_content.encode("utf-8")),
                            "created_at": str(doc.metadata.get("created_at", "")),
                            "updated_at": str(doc.metadata.get("created_at", "")),  # ä½¿ç”¨åˆ›å»ºæ—¶é—´ä½œä¸ºæ›´æ–°æ—¶é—´
                            "chunks_count": chunk_count,
                            "metadata": {
                                "document_id": document_id,
                                "title": doc.metadata.get("title", "Unknown"),
                                "file_type": doc.metadata.get("file_type", "text"),
                                "created_at": doc.metadata.get("created_at", "")
                            }
                        })
            
            print(f"ğŸ“„ æå–åˆ° {len(documents)} ä¸ªå”¯ä¸€æ–‡æ¡£")
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            try:
                documents.sort(key=lambda x: x.get("created_at", ""), reverse=True)
            except:
                pass  # å¦‚æœæ’åºå¤±è´¥ï¼Œä¿æŒåŸæœ‰é¡ºåº
            
            # åˆ†é¡µå¤„ç†
            total_count = len(documents)
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            paginated_docs = documents[start_idx:end_idx]
            
            print(f"ğŸ“Š åˆ†é¡µç»“æœ: æ€»æ•°={total_count}, å½“å‰é¡µ={page}, é¡µå¤§å°={page_size}, è¿”å›={len(paginated_docs)}ä¸ªæ–‡æ¡£")
            
            return DocumentListResponse(
                documents=paginated_docs,
                total_count=total_count,
                page=page,
                page_size=page_size
            )
        except Exception as e:
            print(f"âŒ ä»LangChainå­˜å‚¨è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


    
    async def delete_document(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            # ä»åŸæœ‰å­˜å‚¨åˆ é™¤
            success = await self.storage.delete_document(document_id)
            
            # ä»LangChainå­˜å‚¨åˆ é™¤å‘é‡æ•°æ®
            langchain_success = self.langchain_service.delete_document(document_id)
            
            if success and langchain_success:
                print(f"âœ… æˆåŠŸåˆ é™¤æ–‡æ¡£ {document_id} åŠå…¶å‘é‡æ•°æ®")
                return True
            elif success:
                print(f"âš ï¸ æ–‡æ¡£ {document_id} åˆ é™¤æˆåŠŸï¼Œä½†å‘é‡æ•°æ®åˆ é™¤å¤±è´¥")
                return True
            else:
                print(f"âŒ åˆ é™¤æ–‡æ¡£ {document_id} å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    async def get_document_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–åŸæœ‰å­˜å‚¨ç»Ÿè®¡
            legacy_stats = await self.storage.get_stats()
            
            # è·å–LangChainç»Ÿè®¡
            langchain_stats = self.langchain_service.get_stats()
            
            return {
                "legacy_storage": legacy_stats,
                "langchain_storage": langchain_stats,
                "total_documents": legacy_stats.get("total_documents", 0),
                "total_chunks": langchain_stats.get("total_chunks", 0)
            }
        except Exception as e:
            print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "legacy_storage": {},
                "langchain_storage": {},
                "total_documents": 0,
                "total_chunks": 0
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥åŸæœ‰å­˜å‚¨
            legacy_health = await self.storage.health_check()
            
            # æ£€æŸ¥LangChainå­˜å‚¨
            langchain_health = await self.langchain_service.health_check()
            
            return {
                "status": "healthy" if legacy_health and langchain_health.get("status") == "healthy" else "unhealthy",
                "legacy_storage": {
                    "backend": type(self.storage).__name__,
                    "healthy": legacy_health
                },
                "langchain_storage": langchain_health
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            } 
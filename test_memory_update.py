#!/usr/bin/env python3
"""
éªŒè¯add_documentsæ˜¯å¦ç«‹å³æ›´æ–°å†…å­˜ä¸­çš„å‘é‡
"""

import os
import sys
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.config import settings
from app.services.langchain_service import LangChainRAGService
from langchain.schema import Document


def test_memory_update():
    """æµ‹è¯•å†…å­˜æ›´æ–°"""
    print("ğŸ” æµ‹è¯•add_documentsæ˜¯å¦ç«‹å³æ›´æ–°å†…å­˜ä¸­çš„å‘é‡")
    print("=" * 60)
    
    # åˆå§‹åŒ–æœåŠ¡
    print("ğŸš€ åˆå§‹åŒ–LangChainæœåŠ¡...")
    langchain_service = LangChainRAGService()
    
    if not langchain_service.vector_store:
        print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return
    
    # è·å–åˆå§‹çŠ¶æ€
    initial_count = len(langchain_service.vector_store.index_to_docstore_id)
    print(f"ğŸ“Š åˆå§‹å‘é‡æ•°é‡: {initial_count}")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        Document(
            page_content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯å†…å­˜æ›´æ–°ã€‚",
            metadata={"document_id": "test_001", "title": "æµ‹è¯•æ–‡æ¡£1"}
        ),
        Document(
            page_content="è¿™æ˜¯å¦ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œå†…å®¹ä¸åŒã€‚",
            metadata={"document_id": "test_002", "title": "æµ‹è¯•æ–‡æ¡£2"}
        )
    ]
    
    print(f"\nğŸ“ å‡†å¤‡æ·»åŠ  {len(test_docs)} ä¸ªæµ‹è¯•æ–‡æ¡£...")
    
    # è®°å½•æ·»åŠ å‰çš„æ—¶é—´
    start_time = time.time()
    
    # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
    print("ğŸ’¾ å¼€å§‹æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨...")
    langchain_service.vector_store.add_documents(test_docs)
    
    # è®°å½•æ·»åŠ åçš„æ—¶é—´
    end_time = time.time()
    
    # ç«‹å³æ£€æŸ¥å†…å­˜çŠ¶æ€
    after_add_count = len(langchain_service.vector_store.index_to_docstore_id)
    print(f"ğŸ“Š æ·»åŠ åå‘é‡æ•°é‡: {after_add_count}")
    print(f"ğŸ“ˆ å‘é‡å¢é•¿: {after_add_count - initial_count}")
    print(f"â±ï¸ æ·»åŠ è€—æ—¶: {(end_time - start_time) * 1000:.2f} æ¯«ç§’")
    
    # éªŒè¯å‘é‡æ˜¯å¦çœŸçš„åœ¨å†…å­˜ä¸­
    print(f"\nğŸ” éªŒè¯å‘é‡æ˜¯å¦åœ¨å†…å­˜ä¸­...")
    
    # æ£€æŸ¥FAISSç´¢å¼•
    index = langchain_service.vector_store.index
    print(f"   - FAISSç´¢å¼•å‘é‡æ•°é‡: {index.ntotal}")
    
    # æ£€æŸ¥æ–‡æ¡£å­˜å‚¨
    docstore = langchain_service.vector_store.docstore
    doc_count = len(docstore._dict) if hasattr(docstore, '_dict') else 0
    print(f"   - æ–‡æ¡£å­˜å‚¨æ•°é‡: {doc_count}")
    
    # æ£€æŸ¥æ˜ å°„å…³ç³»
    mapping_count = len(langchain_service.vector_store.index_to_docstore_id)
    print(f"   - ç´¢å¼•æ˜ å°„æ•°é‡: {mapping_count}")
    
    # éªŒè¯ä¸€è‡´æ€§
    print(f"\nâœ… éªŒè¯ç»“æœ:")
    print(f"   - ç´¢å¼•å‘é‡æ•°é‡ = æ˜ å°„æ•°é‡: {'âœ…' if index.ntotal == mapping_count else 'âŒ'}")
    print(f"   - æ–‡æ¡£å­˜å‚¨æ•°é‡ = æ˜ å°„æ•°é‡: {'âœ…' if doc_count == mapping_count else 'âŒ'}")
    
    # ç«‹å³è¿›è¡Œæœç´¢æµ‹è¯•
    print(f"\nğŸ” ç«‹å³è¿›è¡Œæœç´¢æµ‹è¯•...")
    try:
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = langchain_service.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # æœç´¢æµ‹è¯•æ–‡æ¡£
        results = retriever.get_relevant_documents("æµ‹è¯•æ–‡æ¡£")
        print(f"   - æœç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # æ˜¾ç¤ºæœç´¢ç»“æœ
        for i, doc in enumerate(results):
            content_preview = doc.page_content[:50] + "..." if len(doc.page_content) > 50 else doc.page_content
            print(f"     - ç»“æœ {i+1}: {content_preview}")
            print(f"       å…ƒæ•°æ®: {doc.metadata}")
        
        print("âœ… æœç´¢æµ‹è¯•æˆåŠŸï¼Œè¯´æ˜æ–°å‘é‡å·²å¯ç”¨äºæœç´¢")
        
    except Exception as e:
        print(f"âŒ æœç´¢æµ‹è¯•å¤±è´¥: {e}")
    
    # æ£€æŸ¥æ˜¯å¦å·²ä¿å­˜åˆ°ç£ç›˜
    print(f"\nğŸ’¾ æ£€æŸ¥ç£ç›˜æ–‡ä»¶çŠ¶æ€...")
    vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
    index_faiss_path = f"{vector_store_path}/index.faiss"
    
    if os.path.exists(index_faiss_path):
        file_size = os.path.getsize(index_faiss_path)
        print(f"   - index.faiss æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")
        
        # æ³¨æ„ï¼šæ­¤æ—¶æ–‡ä»¶å¯èƒ½è¿˜æ²¡æœ‰æ›´æ–°ï¼Œå› ä¸ºè¿˜æ²¡æœ‰è°ƒç”¨save_local
        print(f"   - æ–‡ä»¶æ˜¯å¦å·²æ›´æ–°: {'éœ€è¦è°ƒç”¨save_local()'}")
    else:
        print(f"   - index.faiss æ–‡ä»¶ä¸å­˜åœ¨")
    
    print(f"\nğŸ’¡ ç»“è®º:")
    print(f"   - add_documents() ç«‹å³æ›´æ–°å†…å­˜ä¸­çš„å‘é‡")
    print(f"   - æ–°å‘é‡ç«‹å³å¯ç”¨äºæœç´¢")
    print(f"   - éœ€è¦è°ƒç”¨ save_local() æ‰ä¼šæ›´æ–°ç£ç›˜æ–‡ä»¶")


def test_save_to_disk():
    """æµ‹è¯•ä¿å­˜åˆ°ç£ç›˜"""
    print(f"\n" + "=" * 60)
    print("ğŸ’¾ æµ‹è¯•ä¿å­˜åˆ°ç£ç›˜")
    print("=" * 60)
    
    langchain_service = LangChainRAGService()
    
    if not langchain_service.vector_store:
        print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return
    
    # è®°å½•ä¿å­˜å‰çš„æ–‡ä»¶å¤§å°
    vector_store_path = f"{settings.data_dir}/faiss/langchain_vectorstore"
    index_faiss_path = f"{vector_store_path}/index.faiss"
    
    if os.path.exists(index_faiss_path):
        before_size = os.path.getsize(index_faiss_path)
        print(f"ğŸ“Š ä¿å­˜å‰æ–‡ä»¶å¤§å°: {before_size / 1024:.2f} KB")
    
    # ä¿å­˜åˆ°ç£ç›˜
    print("ğŸ’¾ ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜...")
    start_time = time.time()
    langchain_service.vector_store.save_local(vector_store_path)
    end_time = time.time()
    
    print(f"â±ï¸ ä¿å­˜è€—æ—¶: {(end_time - start_time) * 1000:.2f} æ¯«ç§’")
    
    # æ£€æŸ¥ä¿å­˜åçš„æ–‡ä»¶å¤§å°
    if os.path.exists(index_faiss_path):
        after_size = os.path.getsize(index_faiss_path)
        print(f"ğŸ“Š ä¿å­˜åæ–‡ä»¶å¤§å°: {after_size / 1024:.2f} KB")
        if 'before_size' in locals():
            print(f"ğŸ“ˆ æ–‡ä»¶å¤§å°å˜åŒ–: {(after_size - before_size) / 1024:.2f} KB")
    
    print("âœ… ä¿å­˜åˆ°ç£ç›˜å®Œæˆ")


if __name__ == "__main__":
    try:
        test_memory_update()
        test_save_to_disk()
        print("\nâœ… æµ‹è¯•å®Œæˆ")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 
#!/usr/bin/env python3
"""
è°ƒè¯•å·¥å…·é›†åˆ
"""

import os
import sys
import asyncio
import json
import time
from typing import Dict, Any, List
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.config import settings
from app.services.langchain_service import LangChainRAGService
from app.services.document_service import DocumentService
from app.services.query_service import QueryService


class DebugTools:
    """è°ƒè¯•å·¥å…·ç±»"""
    
    def __init__(self):
        self.langchain_service = None
        self.document_service = None
        self.query_service = None
    
    async def init_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        print("ğŸš€ åˆå§‹åŒ–æœåŠ¡...")
        
        try:
            self.langchain_service = LangChainRAGService()
            self.document_service = DocumentService()
            self.query_service = QueryService()
            print("âœ… æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def check_environment(self):
        """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
        print("\nğŸ” ç¯å¢ƒæ£€æŸ¥:")
        print("=" * 40)
        
        # æ£€æŸ¥é…ç½®
        print(f"ğŸ“ æ•°æ®ç›®å½•: {settings.data_dir}")
        print(f"ğŸ”‘ DeepSeek API URL: {settings.deepseek_api_url}")
        print(f"ğŸ¤– æ¨¡å‹åç§°: {settings.model_name}")
        
        # æ£€æŸ¥ç›®å½•
        data_dir = Path(settings.data_dir)
        print(f"ğŸ“‚ æ•°æ®ç›®å½•å­˜åœ¨: {data_dir.exists()}")
        
        # æ£€æŸ¥FAISSæ–‡ä»¶
        faiss_dir = data_dir / "faiss" / "langchain_vectorstore"
        print(f"ğŸ“„ FAISSç›®å½•å­˜åœ¨: {faiss_dir.exists()}")
        if faiss_dir.exists():
            index_file = faiss_dir / "index.faiss"
            pkl_file = faiss_dir / "index.pkl"
            print(f"   - index.faiss: {index_file.exists()} ({index_file.stat().st_size if index_file.exists() else 0} bytes)")
            print(f"   - index.pkl: {pkl_file.exists()} ({pkl_file.stat().st_size if pkl_file.exists() else 0} bytes)")
    
    def check_vector_store(self):
        """æ£€æŸ¥å‘é‡å­˜å‚¨çŠ¶æ€"""
        print("\nğŸ“Š å‘é‡å­˜å‚¨æ£€æŸ¥:")
        print("=" * 40)
        
        if not self.langchain_service or not self.langchain_service.vector_store:
            print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
            return
        
        vector_store = self.langchain_service.vector_store
        print(f"ğŸ“„ æ–‡æ¡£å—æ•°é‡: {len(vector_store.index_to_docstore_id)}")
        print(f"ğŸ”¢ FAISSç´¢å¼•å‘é‡æ•°: {vector_store.index.ntotal}")
        print(f"ğŸ“ å‘é‡ç»´åº¦: {vector_store.index.d}")
        
        # æ˜¾ç¤ºæ–‡æ¡£åˆ—è¡¨
        print("\nğŸ“š å·²ç´¢å¼•çš„æ–‡æ¡£:")
        doc_ids = set()
        for idx, doc_id in vector_store.index_to_docstore_id.items():
            doc = vector_store.docstore._dict.get(doc_id)
            if doc:
                doc_id_real = doc.metadata.get('document_id', doc_id)
                doc_ids.add(doc_id_real)
        
        for doc_id in sorted(doc_ids):
            print(f"   - {doc_id}")
    
    async def test_upload(self, file_path: str):
        """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ """
        print(f"\nğŸ“¤ æµ‹è¯•æ–‡æ¡£ä¸Šä¼ : {file_path}")
        print("=" * 40)
        
        if not Path(file_path).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = {
                "title": Path(file_path).stem,
                "content": content,
                "document_id": f"test_{int(time.time())}"
            }
            
            # ä¸Šä¼ åˆ°åŸæœ‰å­˜å‚¨
            print("ğŸ“ ä¸Šä¼ åˆ°åŸæœ‰å­˜å‚¨...")
            success = self.document_service.add_document(document)
            print(f"   ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
            
            # ä¸Šä¼ åˆ°LangChainå­˜å‚¨
            print("ğŸ”— ä¸Šä¼ åˆ°LangChainå­˜å‚¨...")
            success = self.langchain_service.add_documents([document])
            print(f"   ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
            
            # æ£€æŸ¥ä¸Šä¼ åçš„çŠ¶æ€
            self.check_vector_store()
            
        except Exception as e:
            print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    async def test_query(self, query: str):
        """æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½"""
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: '{query}'")
        print("=" * 40)
        
        try:
            # æµ‹è¯•LangChainæŸ¥è¯¢
            print("ğŸ¤– LangChainæŸ¥è¯¢:")
            start_time = time.time()
            result = await self.langchain_service.query(query, top_k=3, threshold=0.3)
            end_time = time.time()
            
            print(f"   è€—æ—¶: {(end_time - start_time) * 1000:.2f} æ¯«ç§’")
            print(f"   ç­”æ¡ˆ: {result.get('answer', '')[:200]}...")
            print(f"   æ¥æºæ•°: {len(result.get('sources', []))}")
            
            # æ˜¾ç¤ºæ¥æº
            sources = result.get('sources', [])
            for i, source in enumerate(sources):
                print(f"   æ¥æº {i+1}: {source.get('content_preview', '')[:80]}...")
            
            # æµ‹è¯•æœç´¢åŠŸèƒ½
            print("\nğŸ” LangChainæœç´¢:")
            start_time = time.time()
            search_results = await self.langchain_service.search_documents(query, top_k=3, threshold=0.1)
            end_time = time.time()
            
            print(f"   è€—æ—¶: {(end_time - start_time) * 1000:.2f} æ¯«ç§’")
            print(f"   ç»“æœæ•°: {len(search_results)}")
            
            for i, result in enumerate(search_results):
                print(f"   ç»“æœ {i+1}: {result.get('document_title', 'Unknown')} (ç›¸ä¼¼åº¦: {result.get('max_similarity', 0):.3f})")
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        print("\nğŸ’¾ å†…å­˜ä½¿ç”¨æ£€æŸ¥:")
        print("=" * 40)
        
        try:
            import psutil
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            print(f"ğŸ“Š ç‰©ç†å†…å­˜: {memory_info.rss / 1024 / 1024:.2f} MB")
            print(f"ğŸ“Š è™šæ‹Ÿå†…å­˜: {memory_info.vms / 1024 / 1024:.2f} MB")
            print(f"ğŸ“Š å†…å­˜ä½¿ç”¨ç‡: {process.memory_percent():.2f}%")
            
            if self.langchain_service and self.langchain_service.vector_store:
                vector_store = self.langchain_service.vector_store
                # ä¼°ç®—å‘é‡å†…å­˜
                vector_memory = vector_store.index.ntotal * vector_store.index.d * 4 / 1024 / 1024
                print(f"ğŸ”¢ å‘é‡å†…å­˜ä¼°ç®—: {vector_memory:.2f} MB")
                
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£… psutil: pip install psutil")
        except Exception as e:
            print(f"âŒ å†…å­˜æ£€æŸ¥å¤±è´¥: {e}")
    
    def export_debug_info(self, filename: str = "debug_info.json"):
        """å¯¼å‡ºè°ƒè¯•ä¿¡æ¯"""
        print(f"\nğŸ“¤ å¯¼å‡ºè°ƒè¯•ä¿¡æ¯åˆ°: {filename}")
        print("=" * 40)
        
        debug_info = {
            "timestamp": time.time(),
            "environment": {
                "data_dir": settings.data_dir,
                "deepseek_api_url": settings.deepseek_api_url,
                "model_name": settings.model_name
            },
            "vector_store": {}
        }
        
        if self.langchain_service and self.langchain_service.vector_store:
            vector_store = self.langchain_service.vector_store
            debug_info["vector_store"] = {
                "document_count": len(vector_store.index_to_docstore_id),
                "index_vectors": vector_store.index.ntotal,
                "vector_dimension": vector_store.index.d,
                "documents": []
            }
            
            # æ”¶é›†æ–‡æ¡£ä¿¡æ¯
            doc_ids = set()
            for idx, doc_id in vector_store.index_to_docstore_id.items():
                doc = vector_store.docstore._dict.get(doc_id)
                if doc:
                    doc_id_real = doc.metadata.get('document_id', doc_id)
                    if doc_id_real not in doc_ids:
                        doc_ids.add(doc_id_real)
                        debug_info["vector_store"]["documents"].append({
                            "document_id": doc_id_real,
                            "title": doc.metadata.get('title', 'Unknown'),
                            "chunk_count": 0
                        })
            
            # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„å—æ•°
            for doc in debug_info["vector_store"]["documents"]:
                doc_id = doc["document_id"]
                chunk_count = sum(1 for idx, stored_doc_id in vector_store.index_to_docstore_id.items()
                                if vector_store.docstore._dict.get(stored_doc_id, {}).metadata.get('document_id') == doc_id)
                doc["chunk_count"] = chunk_count
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(debug_info, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ°: {filename}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ› Tag Demo è°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    debug_tools = DebugTools()
    
    try:
        # åˆå§‹åŒ–æœåŠ¡
        await debug_tools.init_services()
        
        # ç¯å¢ƒæ£€æŸ¥
        debug_tools.check_environment()
        
        # å‘é‡å­˜å‚¨æ£€æŸ¥
        debug_tools.check_vector_store()
        
        # å†…å­˜ä½¿ç”¨æ£€æŸ¥
        debug_tools.check_memory_usage()
        
        # å¯¼å‡ºè°ƒè¯•ä¿¡æ¯
        debug_tools.export_debug_info()
        
        print("\nâœ… è°ƒè¯•æ£€æŸ¥å®Œæˆ")
        
        # äº¤äº’å¼è°ƒè¯•
        print("\nğŸ”§ äº¤äº’å¼è°ƒè¯•é€‰é¡¹:")
        print("1. æµ‹è¯•æ–‡æ¡£ä¸Šä¼ ")
        print("2. æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½")
        print("3. é‡æ–°æ£€æŸ¥çŠ¶æ€")
        print("4. é€€å‡º")
        
        while True:
            choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-4): ").strip()
            
            if choice == "1":
                file_path = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ").strip()
                if file_path:
                    await debug_tools.test_upload(file_path)
            
            elif choice == "2":
                query = input("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹: ").strip()
                if query:
                    await debug_tools.test_query(query)
            
            elif choice == "3":
                debug_tools.check_vector_store()
                debug_tools.check_memory_usage()
            
            elif choice == "4":
                print("ğŸ‘‹ é€€å‡ºè°ƒè¯•å·¥å…·")
                break
            
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    except Exception as e:
        print(f"âŒ è°ƒè¯•å·¥å…·è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 
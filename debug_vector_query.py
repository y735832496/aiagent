#!/usr/bin/env python3
"""
è°ƒè¯•å‘é‡æ•°æ®åº“æŸ¥è¯¢è¿‡ç¨‹
"""

import os
import sys
import asyncio
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.config import settings
from app.services.langchain_service import LangChainRAGService


async def debug_vector_query_process():
    """è°ƒè¯•å‘é‡æ•°æ®åº“æŸ¥è¯¢è¿‡ç¨‹"""
    print("ğŸ” è°ƒè¯•å‘é‡æ•°æ®åº“æŸ¥è¯¢è¿‡ç¨‹")
    print("=" * 60)
    
    # åˆå§‹åŒ–æœåŠ¡
    print("ğŸš€ åˆå§‹åŒ–LangChainæœåŠ¡...")
    langchain_service = LangChainRAGService()
    
    if not langchain_service.vector_store:
        print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return
    
    # æµ‹è¯•æŸ¥è¯¢
    test_query = "ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ"
    print(f"\nğŸ“ ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢: '{test_query}'")
    
    # è¯¦ç»†å±•ç¤ºå‘é‡æŸ¥è¯¢è¿‡ç¨‹
    print("\nğŸ”„ å‘é‡æ•°æ®åº“æŸ¥è¯¢è¿‡ç¨‹:")
    print("=" * 40)
    
    # 1. åˆ›å»ºæ£€ç´¢å™¨
    print("1ï¸âƒ£ åˆ›å»ºæ£€ç´¢å™¨...")
    retriever = langchain_service.vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": 5,
            "score_threshold": 0.3
        }
    )
    print("   âœ… æ£€ç´¢å™¨åˆ›å»ºå®Œæˆ")
    
    # 2. æ‰§è¡Œå‘é‡æŸ¥è¯¢
    print("\n2ï¸âƒ£ æ‰§è¡Œå‘é‡æ•°æ®åº“æŸ¥è¯¢...")
    start_time = time.time()
    
    try:
        # è¿™é‡Œå°±æ˜¯å‘é‡æ•°æ®åº“æŸ¥è¯¢ï¼
        source_documents = await retriever.aget_relevant_documents(test_query)
        
        end_time = time.time()
        print(f"   âœ… å‘é‡æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {(end_time - start_time) * 1000:.2f} æ¯«ç§’")
        print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(source_documents)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # 3. æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ
        print("\n3ï¸âƒ£ å‘é‡æŸ¥è¯¢ç»“æœ:")
        for i, doc in enumerate(source_documents):
            print(f"   ğŸ“„ æ–‡æ¡£ {i+1}:")
            print(f"      - å†…å®¹: {doc.page_content[:100]}...")
            print(f"      - å…ƒæ•°æ®: {doc.metadata}")
        
        # 4. å±•ç¤ºå®Œæ•´çš„RAGæµç¨‹
        print("\n4ï¸âƒ£ å®Œæ•´çš„RAGæµç¨‹:")
        print("   ğŸ“ æ„å»ºä¸Šä¸‹æ–‡...")
        context_parts = [doc.page_content for doc in source_documents]
        context = "\n\n".join(context_parts)
        print(f"      - ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
        
        print("   ğŸ¯ æ„å»ºæç¤ºè¯...")
        prompt_template = langchain_service._get_qa_prompt()
        formatted_prompt = prompt_template.format(
            context=context,
            question=test_query
        )
        print(f"      - æç¤ºè¯é•¿åº¦: {len(formatted_prompt)} å­—ç¬¦")
        
        print("   ğŸ¤– è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ...")
        llm_response = await langchain_service.llm._acall(
            formatted_prompt,
            max_tokens=1000
        )
        print(f"      - LLMå“åº”: {llm_response[:200]}...")
        
        print("\nâœ… å®Œæ•´çš„RAGæµç¨‹å®Œæˆï¼")
        
    except Exception as e:
        print(f"   âŒ å‘é‡æŸ¥è¯¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def debug_retriever_internals():
    """è°ƒè¯•æ£€ç´¢å™¨å†…éƒ¨å®ç°"""
    print("\nğŸ”§ è°ƒè¯•æ£€ç´¢å™¨å†…éƒ¨å®ç°:")
    print("=" * 40)
    
    langchain_service = LangChainRAGService()
    
    if not langchain_service.vector_store:
        print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return
    
    test_query = "å‘é‡æ•°æ®åº“çš„ä¼˜åŠ¿"
    print(f"ğŸ“ æµ‹è¯•æŸ¥è¯¢: '{test_query}'")
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = langchain_service.vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": 3,
            "score_threshold": 0.3
        }
    )
    
    print("\nğŸ” æ£€ç´¢å™¨å†…éƒ¨æ‰§è¡Œè¿‡ç¨‹:")
    
    # 1. æŸ¥è¯¢å‘é‡åŒ–
    print("1ï¸âƒ£ æŸ¥è¯¢å‘é‡åŒ–...")
    query_vector = langchain_service.embeddings.embed_query(test_query)
    print(f"   - æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")
    print(f"   - æŸ¥è¯¢å‘é‡å‰5ä¸ªå€¼: {query_vector[:5]}")
    
    # 2. å‘é‡ç›¸ä¼¼åº¦æœç´¢
    print("\n2ï¸âƒ£ å‘é‡ç›¸ä¼¼åº¦æœç´¢...")
    # è¿™é‡Œæ¨¡æ‹ŸFAISSçš„æœç´¢è¿‡ç¨‹
    index = langchain_service.vector_store.index
    print(f"   - FAISSç´¢å¼•ä¸­çš„å‘é‡æ•°é‡: {index.ntotal}")
    print(f"   - FAISSç´¢å¼•ç»´åº¦: {index.d}")
    
    # 3. æ‰§è¡Œæœç´¢
    print("\n3ï¸âƒ£ æ‰§è¡Œå‘é‡æœç´¢...")
    source_documents = await retriever.aget_relevant_documents(test_query)
    print(f"   - æœç´¢åˆ° {len(source_documents)} ä¸ªç›¸å…³æ–‡æ¡£")
    
    # 4. æ˜¾ç¤ºæœç´¢ç»“æœ
    print("\n4ï¸âƒ£ æœç´¢ç»“æœè¯¦æƒ…:")
    for i, doc in enumerate(source_documents):
        print(f"   ğŸ“„ ç»“æœ {i+1}:")
        print(f"      - å†…å®¹é¢„è§ˆ: {doc.page_content[:80]}...")
        print(f"      - æ–‡æ¡£ID: {doc.metadata.get('document_id', 'N/A')}")
        print(f"      - æ ‡é¢˜: {doc.metadata.get('title', 'N/A')}")


async def compare_with_direct_search():
    """ä¸ç›´æ¥æœç´¢å¯¹æ¯”"""
    print("\nâš–ï¸ ä¸ç›´æ¥æœç´¢å¯¹æ¯”:")
    print("=" * 40)
    
    langchain_service = LangChainRAGService()
    
    if not langchain_service.vector_store:
        print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return
    
    test_query = "å‘é‡æ•°æ®åº“"
    print(f"ğŸ“ æµ‹è¯•æŸ¥è¯¢: '{test_query}'")
    
    # æ–¹æ³•1: ä½¿ç”¨æ£€ç´¢å™¨ï¼ˆainvokeå†…éƒ¨ä½¿ç”¨çš„æ–¹æ³•ï¼‰
    print("\nğŸ” æ–¹æ³•1: ä½¿ç”¨æ£€ç´¢å™¨ (ainvokeå†…éƒ¨æ–¹æ³•)")
    retriever = langchain_service.vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
    
    start_time = time.time()
    retriever_results = await retriever.aget_relevant_documents(test_query)
    retriever_time = time.time() - start_time
    
    print(f"   - æ£€ç´¢å™¨æ–¹æ³•è€—æ—¶: {retriever_time * 1000:.2f} æ¯«ç§’")
    print(f"   - æ£€ç´¢åˆ° {len(retriever_results)} ä¸ªæ–‡æ¡£")
    
    # æ–¹æ³•2: ç›´æ¥ä½¿ç”¨å‘é‡å­˜å‚¨æœç´¢
    print("\nğŸ” æ–¹æ³•2: ç›´æ¥å‘é‡å­˜å‚¨æœç´¢")
    start_time = time.time()
    direct_results = langchain_service.vector_store.similarity_search(test_query, k=3)
    direct_time = time.time() - start_time
    
    print(f"   - ç›´æ¥æœç´¢è€—æ—¶: {direct_time * 1000:.2f} æ¯«ç§’")
    print(f"   - æ£€ç´¢åˆ° {len(direct_results)} ä¸ªæ–‡æ¡£")
    
    # å¯¹æ¯”ç»“æœ
    print("\nğŸ“Š ç»“æœå¯¹æ¯”:")
    print(f"   - æ£€ç´¢å™¨æ–¹æ³•: {len(retriever_results)} ä¸ªæ–‡æ¡£")
    print(f"   - ç›´æ¥æœç´¢: {len(direct_results)} ä¸ªæ–‡æ¡£")
    print(f"   - æ—¶é—´å·®å¼‚: {abs(retriever_time - direct_time) * 1000:.2f} æ¯«ç§’")
    
    # éªŒè¯ç»“æœæ˜¯å¦ä¸€è‡´
    retriever_contents = [doc.page_content[:50] for doc in retriever_results]
    direct_contents = [doc.page_content[:50] for doc in direct_results]
    
    print(f"   - ç»“æœä¸€è‡´æ€§: {'âœ… ä¸€è‡´' if retriever_contents == direct_contents else 'âŒ ä¸ä¸€è‡´'}")


async def test_full_query_flow():
    """æµ‹è¯•å®Œæ•´çš„queryæµç¨‹"""
    print("\nğŸ§ª æµ‹è¯•å®Œæ•´çš„queryæµç¨‹:")
    print("=" * 40)
    
    langchain_service = LangChainRAGService()
    
    if not langchain_service.vector_store:
        print("âŒ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return
    
    test_query = "å‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    print(f"ğŸ“ æµ‹è¯•æŸ¥è¯¢: '{test_query}'")
    
    try:
        # ä½¿ç”¨å®Œæ•´çš„queryæ–¹æ³•
        start_time = time.time()
        result = await langchain_service.query(
            query=test_query,
            top_k=3,
            threshold=0.3
        )
        end_time = time.time()
        
        print(f"âœ… å®Œæ•´queryæµç¨‹å®Œæˆï¼Œè€—æ—¶: {(end_time - start_time) * 1000:.2f} æ¯«ç§’")
        print(f"ğŸ“„ ç”Ÿæˆçš„ç­”æ¡ˆ: {result.get('answer', '')[:200]}...")
        print(f"ğŸ“š æ¥æºæ–‡æ¡£æ•°: {len(result.get('sources', []))}")
        
        # æ˜¾ç¤ºæ¥æºä¿¡æ¯
        sources = result.get('sources', [])
        for i, source in enumerate(sources):
            print(f"   ğŸ“„ æ¥æº {i+1}: {source.get('content_preview', '')[:80]}...")
        
    except Exception as e:
        print(f"âŒ queryæµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    try:
        asyncio.run(debug_vector_query_process())
        asyncio.run(debug_retriever_internals())
        asyncio.run(compare_with_direct_search())
        asyncio.run(test_full_query_flow())
        print("\nâœ… è°ƒè¯•å®Œæˆ")
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 